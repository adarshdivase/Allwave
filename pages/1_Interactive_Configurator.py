import streamlit as st
import pandas as pd
import os
import glob
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import warnings
import re
from typing import List, Dict, Any, Generator
import fitz  # PyMuPDF
from datetime import datetime, timedelta
import random
from dataclasses import dataclass
import google.generativeai as genai
import mailbox
from email import policy
from email.parser import BytesParser

warnings.filterwarnings("ignore")
st.set_page_config(
    page_title="AI Support Assistant",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Settings ---
@dataclass
class RAGConfig:
    chunk_size: int = 500
    top_k_retrieval: int = 5
    similarity_threshold: float = 0.4

if "settings" not in st.session_state:
    st.session_state.settings = {"chunk_size": 500, "theme": "light"}

config = RAGConfig(chunk_size=st.session_state.settings["chunk_size"])

# --- Predictive Maintenance Integration (Simulation) ---
class MaintenancePipeline:
    def __init__(self):
        self.maintenance_data = self._load_maintenance_data()

    def _load_maintenance_data(self) -> Dict:
        equipment_data = {}
        equipment_types = ['HVAC', 'IT_EQUIPMENT', 'ELECTRICAL', 'FIRE_SAFETY']
        for i in range(20):
            eq_type = random.choice(equipment_types)
            fail_prob = random.uniform(0.1, 0.9)
            equipment_data[f"{eq_type}_{i+1}"] = {
                'type': eq_type,
                'location': f"Building A - Floor {random.randint(1,4)}",
                'failure_probability': fail_prob,
                'risk_level': 'HIGH' if fail_prob > 0.7 else 'MEDIUM' if fail_prob > 0.4 else 'LOW',
                'next_maintenance': (datetime.now() + timedelta(days=random.randint(7, 90))).strftime('%Y-%m-%d'),
                'maintenance_cost': random.randint(200, 5000)
            }
        return equipment_data

    def get_equipment_by_risk(self, risk_level: str) -> List[Dict]:
        return [{'id': eid, **edata} for eid, edata in self.maintenance_data.items() if edata['risk_level'] == risk_level.upper()]

    def get_maintenance_schedule(self, days_ahead: int = 30) -> List[Dict]:
        target_date = datetime.now() + timedelta(days=days_ahead)
        items = [{'id': eid, **edata} for eid, edata in self.maintenance_data.items() if datetime.strptime(edata['next_maintenance'], '%Y-%m-%d') <= target_date]
        return sorted(items, key=lambda x: x['next_maintenance'])

# --- Tool 1: Data Analysis Tool ---
def analyze_csv_data(query: str) -> str:
    try:
        jira_file = next(f for f in glob.glob("*.csv") if "Closed tickets" in f)
    except StopIteration:
        return "The Jira ticket export CSV file was not found."
    try:
        df = pd.read_csv(jira_file)
        query_lower = query.lower()
        match = re.search(r"count of (.+?) issues|how many (.+?) issues", query_lower)
        if not match:
             match = re.search(r"related to a '(.+?)' issue", query_lower)
        if match:
            target_value = next(g for g in match.groups() if g is not None).strip()
            target_column = None
            for col in df.columns:
                if "rca" in col.lower() or "root cause" in col.lower():
                    target_column = col
                    break
            if not target_column:
                return "Could not determine the 'Root Cause Analysis' column in the CSV."
            count = df[df[target_column].str.contains(target_value, case=False, na=False)].shape[0]
            return f"Found **{count} tickets** related to **'{target_value}'**."
        return "I can count issues from the Jira file, but I didn't understand what to count. Try 'how many Device Faulty issues?'"
    except Exception as e:
        return f"An error occurred during CSV analysis: {e}"

# --- Tool 2: Maintenance and RAG Tool ---
def get_maintenance_context_with_actions(query: str, maintenance_pipeline: MaintenancePipeline, search_func, search_args) -> str:
    query_lower = query.lower()
    context_parts = []
    if any(kw in query_lower for kw in ['risk', 'failure', 'alert']):
        high_risk = maintenance_pipeline.get_equipment_by_risk('HIGH')
        if high_risk:
            context_parts.append("### High-Risk Equipment Alerts:\n")
            for eq in high_risk[:3]:
                context_parts.append(f"- **ID:** `{eq['id']}` | **Location:** {eq['location']} | **Failure Probability:** {eq['failure_probability']:.1%}")
                action_query = f"troubleshooting procedure for {eq['type']}"
                action_results = search_func(action_query, **search_args)
                if action_results:
                    action_text = action_results[0]['content'].strip().replace('\n', ' ')
                    source = os.path.basename(action_results[0]['source'])
                    context_parts.append(f"  - **â–¶ï¸ Recommended Action:** Based on `{source}`, you should: *\"{action_text[:200]}...\"*")
                else:
                    context_parts.append("  - *No specific troubleshooting guide found in the documents.*")
    elif any(kw in query_lower for kw in ['schedule', 'calendar', 'upcoming']):
        schedule = maintenance_pipeline.get_maintenance_schedule(30)
        if schedule:
            context_parts.append("### Upcoming Maintenance (Next 30 Days):\n")
            for item in schedule[:5]:
                context_parts.append(f"- **{item['next_maintenance']}**: `{item['id']}` at {item['location']} (Cost: ${item['maintenance_cost']})")
    return "\n".join(context_parts)

# --- LLM and Response Generation (Upgraded) ---
try:
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    GEMINI_MODEL = genai.GenerativeModel('gemini-1.5-flash')
except Exception as e:
    st.error(f"Error configuring Gemini API: {e}")
    GEMINI_MODEL = None

def summarize_history(history: List[Dict], max_tokens=300) -> str:
    text = ""
    for msg in history[-6:]:
        text += f"{msg['role'].capitalize()}: {msg['content']}\n"
    return text[-max_tokens:]

def generate_response_stream(query: str, chat_history: List[Dict], context: str) -> Generator:
    if not GEMINI_MODEL:
        yield "The AI model is not configured. Please check your API key."
        return
    history_prompt = summarize_history(chat_history)
    prompt = f"""You are an expert AI support assistant. Your goal is to provide clear, concise answers.
Use the chat history for context but rely ONLY on the 'Context for your answer' to formulate your response. Cite sources if available.

---
**Chat History:**
{history_prompt if history_prompt else "This is the start of the conversation."}
---
**Context for your answer:**
{context if context else "No specific context was found for this query."}
---
**User's Question:** "{query}"
---
Based on all the information above, provide a direct and helpful answer. If the context is empty, explain what the user can ask about."""
    try:
        response_stream = GEMINI_MODEL.generate_content(prompt, stream=True)
        for chunk in response_stream:
            yield chunk.text
    except Exception as e:
        yield f"An error occurred while generating the response: {e}"

# --- Document Processing and Search ---
def clean_text(text: str) -> str:
    return re.sub(r'\s+', ' ', text.strip())

def smart_chunking(text: str, chunk_size: int) -> List[str]:
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < chunk_size:
            current_chunk += sentence + " "
        else:
            if current_chunk: chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    if current_chunk: chunks.append(current_chunk.strip())
    return [c for c in chunks if len(c) > 30]

def extract_text_from_pdf(file_path: str) -> str:
    try:
        with fitz.open(file_path) as doc:
            return "".join(page.get_text() for page in doc)
    except Exception as e:
        st.warning(f"Could not read {os.path.basename(file_path)} with PyMuPDF: {e}")
        return ""

def extract_text_from_email(msg) -> str:
    if msg.is_multipart():
        for part in msg.walk():
            if part.get_content_type() == "text/plain" and "attachment" not in str(part.get("Content-Disposition")):
                try: return part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8')
                except: continue
    elif msg.get_content_type() == "text/plain":
        try: return msg.get_payload(decode=True).decode(msg.get_content_charset() or 'utf-8')
        except: return ""
    return ""

def format_email_for_rag(msg) -> str:
    body = extract_text_from_email(msg)
    if not body: return ""
    return f"--- Email ---\nFrom: {msg['From']}\nTo: {msg['To']}\nSubject: {msg['Subject']}\nDate: {msg['Date']}\n{body.strip()}\n--- End Email ---"

@st.cache_resource
def load_and_process_documents():
    file_patterns = ["**/*.txt", "**/*.md", "**/*.csv", "**/*.pdf", "**/*.eml", "**/*.mbox"]
    all_files = [f for pattern in file_patterns for f in glob.glob(pattern, recursive=True)]
    docs, file_paths = [], []
    progress_bar = st.progress(0, text="Loading documents...")
    for i, file_path in enumerate(all_files):
        file_name = os.path.basename(file_path)
        progress_bar.progress((i + 1) / len(all_files), text=f"Processing: {file_name}")
        content = ""
        if file_path.endswith('.pdf'): content = extract_text_from_pdf(file_path)
        elif file_path.endswith('.eml'):
            with open(file_path, 'rb') as f: content = format_email_for_rag(BytesParser(policy=policy.default).parse(f))
        elif file_path.endswith('.mbox'):
            mbox_content = [format_email_for_rag(msg) for msg in mailbox.mbox(file_path)]
            content = "\n\n".join(filter(None, mbox_content))
        else:
            try:
                with open(file_path, 'r', encoding='utf-8') as f: content = f.read()
            except:
                with open(file_path, 'r', encoding='latin-1') as f: content = f.read()
        if content and len(content.strip()) > 50:
            docs.append(clean_text(content))
            file_paths.append(file_path)
    progress_bar.empty()
    if docs: st.success(f"Successfully loaded and processed {len(docs)} documents!")
    return docs, file_paths

@st.cache_resource
def create_search_index(_documents, _file_paths):
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    all_chunks, chunk_metadata = [], []
    for i, doc in enumerate(_documents):
        chunks = smart_chunking(doc, config.chunk_size)
        all_chunks.extend(chunks)
        chunk_metadata.extend([{'path': _file_paths[i]}] * len(chunks))
    if not all_chunks: return None, None, [], []
    with st.spinner("Embedding documents for semantic search..."):
        embeddings = model.encode(all_chunks, show_progress_bar=True, normalize_embeddings=True)
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings.astype('float32'))
    st.success(f"âœ… Search index created with {len(all_chunks)} text chunks.")
    return index, model, all_chunks, chunk_metadata

def search_documents(query: str, index, model, chunks: List[str], metadata: List[Dict]) -> List[Dict]:
    if not query or index is None: return []
    query_embedding = model.encode([query], normalize_embeddings=True)
    scores, indices = index.search(query_embedding.astype('float32'), config.top_k_retrieval)
    results = []
    for i, idx in enumerate(indices[0]):
        if idx != -1 and scores[0][i] > config.similarity_threshold:
             results.append({'content': chunks[idx], 'source': metadata[idx]['path'], 'similarity': scores[0][i]})
    return sorted(results, key=lambda x: x['similarity'], reverse=True)

# --- Intent Classification ---
def classify_intent(prompt):
    prompt = prompt.lower()
    if any(kw in prompt for kw in ["how many", "count", "total tickets"]):
        return "csv"
    if any(kw in prompt for kw in ['risk', 'failure', 'alert', 'schedule', 'calendar', 'upcoming']):
        return "maintenance"
    return "search"

# --- Initialization ---
# Moved up to ensure these are available for the sidebar
try:
    maintenance_pipeline = MaintenancePipeline()
    docs, paths = load_and_process_documents()
    if docs:
        search_index, model, all_chunks, chunk_meta = create_search_index(docs, paths)
        search_args = {'index': search_index, 'model': model, 'chunks': all_chunks, 'metadata': chunk_meta}
    else:
        search_index, model, all_chunks, chunk_meta, search_args = None, None, [], [], {}
        st.warning("No documents found. Document search and actionable insights are disabled.")
except Exception as e:
    st.error(f"An error occurred during initialization: {e}")
    search_index, search_args = None, {}

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Hi! How can I help you today?", "timestamp": datetime.now().strftime("%H:%M")}]


# --- Sidebar: Quick Actions, Upload, Settings ---
with st.sidebar:
    st.title("ğŸ¤– AI Support Assistant")
    st.markdown("**Quick Actions:**")

    # Handler function for quick actions
    def handle_quick_action(action_type: str):
        if action_type == "alerts":
            prompt = "Show high-risk equipment alerts"
        else:
            prompt = "Show upcoming maintenance"
            
        now = datetime.now().strftime("%H:%M")
        # Add user message
        st.session_state.messages.append({
            "role": "user",
            "content": prompt,
            "timestamp": now
        })
        
        # Generate AI response context
        if action_type == "alerts":
            context = get_maintenance_context_with_actions(
                prompt, 
                maintenance_pipeline,
                search_documents,
                search_args
            )
        else:
            context = get_maintenance_context_with_actions(
                prompt,
                maintenance_pipeline,
                search_documents,
                search_args
            )
            
        # Generate and add AI response
        response_stream = generate_response_stream(prompt, st.session_state.messages, context)
        full_response = "".join([chunk for chunk in response_stream])
        st.session_state.messages.append({
            "role": "assistant",
            "content": full_response,
            "timestamp": now
        })
        # Force a rerun to show the new messages
        st.rerun()

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ“Š Risk Alerts", key="risk_btn", help="Show equipment with the highest failure probability."):
            handle_quick_action("alerts")
    with col2:
        if st.button("ğŸ“… Maintenance", key="maint_btn", help="Show the upcoming maintenance schedule."):
            handle_quick_action("maintenance")

    # Rest of your sidebar code
    st.markdown("---")
    st.markdown("**System Status:**")
    st.write(f"ğŸš¨ {len(maintenance_pipeline.get_equipment_by_risk('HIGH'))} high risk items")
    st.write(f"ğŸ“… {len(maintenance_pipeline.get_maintenance_schedule(7))} tasks due this week")
    st.markdown("---")
    st.info("Upload new documents below:")
    uploaded_files = st.file_uploader("Add files", accept_multiple_files=True)
    if uploaded_files:
        for uploaded_file in uploaded_files:
            # Save to a directory for persistent storage if needed
            with open(uploaded_file.name, "wb") as f:
                f.write(uploaded_file.getbuffer())
        st.success("Files uploaded! Please reload the app to re-index documents.")
        st.button("Reload App")
    st.markdown("---")
    st.markdown("**Settings:**")
    chunk_size = st.slider("Chunk size for document splitting", 200, 1000, st.session_state.settings["chunk_size"], 100)
    st.session_state.settings["chunk_size"] = chunk_size
    theme = st.radio("Theme", ["light", "dark"], index=0 if st.session_state.settings["theme"]=="light" else 1)
    st.session_state.settings["theme"] = theme

# --- Improved Chat Bubble with Avatar and Timestamp ---
def chat_bubble(content, is_user=False, timestamp=None):
    theme = st.session_state.settings.get("theme", "light")
    if theme == "dark":
        user_color = "#3A6351"
        assistant_color = "#222831"
        text_color = "#fff"
        border = "1px solid #393E46"
    else:
        user_color = "#DCF8C6"
        assistant_color = "#F1F0F0"
        text_color = "#222"
        border = "1px solid #ddd"
    
    color = user_color if is_user else assistant_color
    align = "right" if is_user else "left"
    avatar_url = (
        "https://cdn-icons-png.flaticon.com/512/1946/1946429.png" if is_user
        else "https://cdn-icons-png.flaticon.com/512/4712/4712035.png"
    )
    name = "You" if is_user else "AI"
    if not timestamp:
        timestamp = datetime.now().strftime("%H:%M")
    
    # Sanitize content for HTML display
    content = content.replace("<", "&lt;").replace(">", "&gt;")

    st.markdown(
        f"""
        <div style='display:flex;flex-direction:{"row-reverse" if is_user else "row"};align-items:flex-end;margin:8px 0;'>
            <img src="{avatar_url}" width="36" height="36" style="border-radius:50%;margin:0 8px;">
            <div style='background-color:{color};
                        color:{text_color};
                        padding:14px 16px;
                        border-radius:12px;
                        max-width:70vw;
                        min-width:80px;
                        border:{border};
                        box-shadow:0 2px 8px rgba(0,0,0,0.07);
                        position:relative;'>
                <div style='font-size:13px;font-weight:bold;opacity:0.7;'>{name}</div>
                <div style='font-size:16px;margin:4px 0;'>{content}</div>
                <div style='font-size:11px;opacity:0.5;text-align:right;'>{timestamp}</div>
            </div>
        </div>
        """, unsafe_allow_html=True
    )

# --- Feedback Buttons ---
def feedback_buttons():
    col1, col2 = st.columns([1,1])
    with col1:
        if st.button("ğŸ‘", key="thumbs_up"):
            st.success("Thanks for your feedback!")
    with col2:
        if st.button("ğŸ‘", key="thumbs_down"):
            st.warning("We'll try to improve!")


# --- Chat History Display with Avatars and Timestamps ---
for message in st.session_state.messages:
    if "timestamp" not in message:
        message["timestamp"] = datetime.now().strftime("%H:%M")
    chat_bubble(
        message["content"],
        is_user=(message["role"] == "user"),
        timestamp=message["timestamp"]
    )

# --- Chat Input and Tool Routing ---
if prompt := st.chat_input("Ask a question..."):
    now = datetime.now().strftime("%H:%M")
    st.session_state.messages.append({"role": "user", "content": prompt, "timestamp": now})
    # Display the user's message immediately
    chat_bubble(prompt, is_user=True, timestamp=now)
    
    with st.spinner("Thinking..."):
        intent = classify_intent(prompt)
        context = ""
        
        if intent == "csv":
            context = analyze_csv_data(prompt)
        elif intent == "maintenance":
            if search_args:
                context = get_maintenance_context_with_actions(prompt, maintenance_pipeline, search_documents, search_args)
            else:
                context = "Maintenance data is available, but I can't provide actionable insights without documents."
        else: # "search" intent
            if search_args:
                search_results = search_documents(prompt, **search_args)
                if search_results:
                    context += "### Relevant Information from Documents:\n"
                    for result in search_results[:3]:
                        source = os.path.basename(result['source'])
                        content_preview = result['content'].strip().replace('\n', ' ')
                        context += f"- **From `{source}`**: \"{content_preview}\"\n"
                    context += "\n_Citations provided above._"
        
        if not context:
            high_risk_count = len(maintenance_pipeline.get_equipment_by_risk('HIGH'))
            upcoming_maintenance = len(maintenance_pipeline.get_maintenance_schedule(7))
            context = f"""I couldn't find specific information for that query.
- You can ask about document contents (e.g., "summarize the INDIS meeting").
- You can ask for maintenance alerts or schedules.
- You can ask to count Jira tickets (e.g., "how many Device Faulty issues?").

**Current System Status:** ğŸš¨ {high_risk_count} high risk items | ğŸ“… {upcoming_maintenance} tasks due this week."""

        # Generate and display the AI's response
        with st.empty():
            response_stream = generate_response_stream(prompt, st.session_state.messages, context)
            full_response = ""
            for chunk in response_stream:
                full_response += chunk
                chat_bubble(full_response + " â–Œ", is_user=False, timestamp=now)
            
            # Final update to remove the cursor
            chat_bubble(full_response, is_user=False, timestamp=now)
        
        st.session_state.messages.append({"role": "assistant", "content": full_response, "timestamp": now})
        feedback_buttons()
        st.rerun()



        import streamlit as st

import pandas as pd

import os

import glob

from sentence_transformers import SentenceTransformer

import faiss

import numpy as np

import warnings

import re

from typing import List, Dict, Any, Generator

import fitzÂ  # PyMuPDF

from datetime import datetime, timedelta

import random

from dataclasses import dataclass

import google.generativeai as genai

import mailbox

from email import policy

from email.parser import BytesParser



warnings.filterwarnings("ignore")

st.set_page_config(

Â  Â  page_title="AI Support Assistant",

Â  Â  page_icon="ğŸ¤–",

Â  Â  layout="wide",

Â  Â  initial_sidebar_state="expanded"

)



# --- Settings ---

@dataclass

class RAGConfig:

Â  Â  chunk_size: int = 500

Â  Â  top_k_retrieval: int = 5

Â  Â  similarity_threshold: float = 0.4



if "settings" not in st.session_state:

Â  Â  st.session_state.settings = {"chunk_size": 500, "theme": "light"}



config = RAGConfig(chunk_size=st.session_state.settings["chunk_size"])



# --- Predictive Maintenance Integration (Simulation) ---

class MaintenancePipeline:

Â  Â  def __init__(self):

Â  Â  Â  Â  self.maintenance_data = self._load_maintenance_data()



Â  Â  def _load_maintenance_data(self) -> Dict:

Â  Â  Â  Â  # Set random seed for consistent demo data

Â  Â  Â  Â  random.seed(42)

Â  Â  Â  Â  equipment_data = {}

Â  Â  Â  Â  equipment_types = ['HVAC', 'IT_EQUIPMENT', 'ELECTRICAL', 'FIRE_SAFETY']

Â  Â  Â  Â  for i in range(20):

Â  Â  Â  Â  Â  Â  eq_type = random.choice(equipment_types)

Â  Â  Â  Â  Â  Â  fail_prob = random.uniform(0.1, 0.9)

Â  Â  Â  Â  Â  Â  equipment_data[f"{eq_type}_{i+1}"] = {

Â  Â  Â  Â  Â  Â  Â  Â  'type': eq_type,

Â  Â  Â  Â  Â  Â  Â  Â  'location': f"Building A - Floor {random.randint(1,4)}",

Â  Â  Â  Â  Â  Â  Â  Â  'failure_probability': fail_prob,

Â  Â  Â  Â  Â  Â  Â  Â  'risk_level': 'HIGH' if fail_prob > 0.7 else 'MEDIUM' if fail_prob > 0.4 else 'LOW',

Â  Â  Â  Â  Â  Â  Â  Â  'next_maintenance': (datetime.now() + timedelta(days=random.randint(7, 90))).strftime('%Y-%m-%d'),

Â  Â  Â  Â  Â  Â  Â  Â  'maintenance_cost': random.randint(200, 5000)

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  return equipment_data



Â  Â  def get_equipment_by_risk(self, risk_level: str) -> List[Dict]:

Â  Â  Â  Â  return [{'id': eid, **edata} for eid, edata in self.maintenance_data.items() if edata['risk_level'] == risk_level.upper()]



Â  Â  def get_maintenance_schedule(self, days_ahead: int = 30) -> List[Dict]:

Â  Â  Â  Â  target_date = datetime.now() + timedelta(days=days_ahead)

Â  Â  Â  Â  items = [{'id': eid, **edata} for eid, edata in self.maintenance_data.items() if datetime.strptime(edata['next_maintenance'], '%Y-%m-%d') <= target_date]

Â  Â  Â  Â  return sorted(items, key=lambda x: x['next_maintenance'])



# --- Tool 1: Data Analysis Tool ---

def analyze_csv_data(query: str) -> str:

Â  Â  try:

Â  Â  Â  Â  jira_files = [f for f in glob.glob("*.csv") if "Closed tickets" in f or "jira" in f.lower() or "tickets" in f.lower()]

Â  Â  Â  Â  if not jira_files:

Â  Â  Â  Â  Â  Â  return "No Jira ticket CSV files found. Please ensure the CSV file contains 'Closed tickets' in the filename or upload a tickets CSV file."

Â  Â  Â  Â Â 

Â  Â  Â  Â  jira_file = jira_files[0]

Â  Â  Â  Â  df = pd.read_csv(jira_file)

Â  Â  Â  Â Â 

Â  Â  Â  Â  query_lower = query.lower()

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Enhanced pattern matching for various query formats

Â  Â  Â  Â  patterns = [

Â  Â  Â  Â  Â  Â  r"count of (.+?) issues",

Â  Â  Â  Â  Â  Â  r"how many (.+?) issues",

Â  Â  Â  Â  Â  Â  r"(.+?) issue count",

Â  Â  Â  Â  Â  Â  r"related to a?n? '(.+?)' issue",

Â  Â  Â  Â  Â  Â  r"(.+?) tickets"

Â  Â  Â  Â  ]

Â  Â  Â  Â Â 

Â  Â  Â  Â  target_value = None

Â  Â  Â  Â  for pattern in patterns:

Â  Â  Â  Â  Â  Â  match = re.search(pattern, query_lower)

Â  Â  Â  Â  Â  Â  if match:

Â  Â  Â  Â  Â  Â  Â  Â  target_value = next(g for g in match.groups() if g is not None).strip()

Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â Â 

Â  Â  Â  Â  if not target_value:

Â  Â  Â  Â  Â  Â  return f"I found the CSV file '{os.path.basename(jira_file)}' but couldn't understand what to count. Try asking 'How many Device Faulty issues?' or 'Count of Network issues'."

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Find the appropriate column for analysis

Â  Â  Â  Â  target_column = None

Â  Â  Â  Â  possible_columns = ['root cause analysis', 'rca', 'issue type', 'category', 'problem type', 'cause']

Â  Â  Â  Â  for col in df.columns:

Â  Â  Â  Â  Â  Â  if any(pc in col.lower() for pc in possible_columns):

Â  Â  Â  Â  Â  Â  Â  Â  target_column = col

Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â Â 

Â  Â  Â  Â  if not target_column:

Â  Â  Â  Â  Â  Â  available_cols = ", ".join(df.columns[:5])

Â  Â  Â  Â  Â  Â  return f"Could not find a suitable analysis column in the CSV. Available columns include: {available_cols}..."

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Perform the search and count

Â  Â  Â  Â  matching_rows = df[df[target_column].str.contains(target_value, case=False, na=False)]

Â  Â  Â  Â  count = matching_rows.shape[0]

Â  Â  Â  Â Â 

Â  Â  Â  Â  if count > 0:

Â  Â  Â  Â  Â  Â  return f"Found **{count} tickets** related to **'{target_value}'** in the '{target_column}' column."

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  return f"No tickets found related to **'{target_value}'** in the '{target_column}' column. The file contains {len(df)} total tickets."

Â  Â  Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  return f"An error occurred during CSV analysis: {str(e)}. Please ensure the CSV file is properly formatted."



# --- Tool 2: Maintenance and RAG Tool ---

def get_maintenance_context_with_actions(query: str, maintenance_pipeline: MaintenancePipeline, search_func=None, search_args=None) -> str:

Â  Â  query_lower = query.lower()

Â  Â  context_parts = []

Â  Â Â 

Â  Â  if any(kw in query_lower for kw in ['risk', 'failure', 'alert']):

Â  Â  Â  Â  high_risk = maintenance_pipeline.get_equipment_by_risk('HIGH')

Â  Â  Â  Â  if high_risk:

Â  Â  Â  Â  Â  Â  context_parts.append("### High-Risk Equipment Alerts:\n")

Â  Â  Â  Â  Â  Â  for eq in high_risk[:3]:

Â  Â  Â  Â  Â  Â  Â  Â  context_parts.append(f"- **ID:** `{eq['id']}` | **Location:** {eq['location']} | **Failure Probability:** {eq['failure_probability']:.1%}")

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Only attempt to search for actions if search functionality is available

Â  Â  Â  Â  Â  Â  Â  Â  if search_func and search_args:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  action_query = f"troubleshooting procedure for {eq['type']}"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  action_results = search_func(action_query, **search_args)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if action_results:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  action_text = action_results[0]['content'].strip().replace('\n', ' ')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source = os.path.basename(action_results[0]['source'])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context_parts.append(f"Â  - **â–¶ï¸ Recommended Action:** Based on `{source}`: *\"{action_text[:200]}...\"*")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context_parts.append("Â  - *No specific troubleshooting guide found in documents.*")

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context_parts.append("Â  - *Document search not available for specific procedures.*")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  elif any(kw in query_lower for kw in ['schedule', 'calendar', 'upcoming']):

Â  Â  Â  Â  schedule = maintenance_pipeline.get_maintenance_schedule(30)

Â  Â  Â  Â  if schedule:

Â  Â  Â  Â  Â  Â  context_parts.append("### Upcoming Maintenance (Next 30 Days):\n")

Â  Â  Â  Â  Â  Â  for item in schedule[:5]:

Â  Â  Â  Â  Â  Â  Â  Â  context_parts.append(f"- **{item['next_maintenance']}**: `{item['id']}` at {item['location']} (Cost: ${item['maintenance_cost']})")

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  context_parts.append("### No maintenance scheduled for the next 30 days.")

Â  Â Â 

Â  Â  return "\n".join(context_parts)



# --- LLM and Response Generation (Upgraded) ---

try:

Â  Â  genai.configure(api_key=st.secrets["GEMINI_API_KEY"])

Â  Â  GEMINI_MODEL = genai.GenerativeModel('gemini-1.5-flash')

except Exception as e:

Â  Â  st.error(f"Error configuring Gemini API: {e}")

Â  Â  GEMINI_MODEL = None



def summarize_history(history: List[Dict], max_tokens=300) -> str:

Â  Â  text = ""

Â  Â  for msg in history[-6:]:

Â  Â  Â  Â  text += f"{msg['role'].capitalize()}: {msg['content']}\n"

Â  Â  return text[-max_tokens:]



def generate_response_stream(query: str, chat_history: List[Dict], context: str) -> Generator:

Â  Â  if not GEMINI_MODEL:

Â  Â  Â  Â  yield "The AI model is not configured. Please check your API key."

Â  Â  Â  Â  return

Â  Â Â 

Â  Â  history_prompt = summarize_history(chat_history)

Â  Â  # This prompt encourages markdown for better readability, taken from the previous version.

Â  Â  prompt = f"""You are an expert AI support assistant. Your goal is to provide clear, concise answers.

Use the chat history for context but rely ONLY on the 'Context for your answer' to formulate your response. Cite sources if available.



---

**Chat History:**

{history_prompt if history_prompt else "This is the start of the conversation."}

---

**Context for your answer:**

{context if context else "No specific context was found for this query."}

---

**User's Question:** "{query}"

---

Based on all the information above, provide a direct and helpful answer. If the context is empty, explain what the user can ask about."""



Â  Â  try:

Â  Â  Â  Â  response_stream = GEMINI_MODEL.generate_content(prompt, stream=True)

Â  Â  Â  Â  for chunk in response_stream:

Â  Â  Â  Â  Â  Â  if chunk.text:Â  # Ensure chunk has text content

Â  Â  Â  Â  Â  Â  Â  Â  yield chunk.text

Â  Â  except Exception as e:

Â  Â  Â  Â  yield f"An error occurred while generating the response: {e}"



# --- Document Processing and Search ---

def clean_text(text: str) -> str:

Â  Â  return re.sub(r'\s+', ' ', text.strip())



def smart_chunking(text: str, chunk_size: int) -> List[str]:

Â  Â  sentences = re.split(r'(?<=[.!?])\s+', text)

Â  Â  chunks, current_chunk = [], ""

Â  Â  for sentence in sentences:

Â  Â  Â  Â  if len(current_chunk) + len(sentence) < chunk_size:

Â  Â  Â  Â  Â  Â  current_chunk += sentence + " "

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  if current_chunk:Â 

Â  Â  Â  Â  Â  Â  Â  Â  chunks.append(current_chunk.strip())

Â  Â  Â  Â  Â  Â  current_chunk = sentence + " "

Â  Â  if current_chunk:Â 

Â  Â  Â  Â  chunks.append(current_chunk.strip())

Â  Â  return [c for c in chunks if len(c) > 30]



def extract_text_from_pdf(file_path: str) -> str:

Â  Â  try:

Â  Â  Â  Â  with fitz.open(file_path) as doc:

Â  Â  Â  Â  Â  Â  return "".join(page.get_text() for page in doc)

Â  Â  except Exception as e:

Â  Â  Â  Â  st.warning(f"Could not read {os.path.basename(file_path)} with PyMuPDF: {e}")

Â  Â  Â  Â  return ""



def extract_text_from_email(msg) -> str:

Â  Â  if msg.is_multipart():

Â  Â  Â  Â  for part in msg.walk():

Â  Â  Â  Â  Â  Â  if part.get_content_type() == "text/plain" and "attachment" not in str(part.get("Content-Disposition")):

Â  Â  Â  Â  Â  Â  Â  Â  try:Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8')

Â  Â  Â  Â  Â  Â  Â  Â  except:Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  elif msg.get_content_type() == "text/plain":

Â  Â  Â  Â  try:Â 

Â  Â  Â  Â  Â  Â  return msg.get_payload(decode=True).decode(msg.get_content_charset() or 'utf-8')

Â  Â  Â  Â  except:Â 

Â  Â  Â  Â  Â  Â  return ""

Â  Â  return ""



def format_email_for_rag(msg) -> str:

Â  Â  body = extract_text_from_email(msg)

Â  Â  if not body:Â 

Â  Â  Â  Â  return ""

Â  Â  return f"--- Email ---\nFrom: {msg['From']}\nTo: {msg['To']}\nSubject: {msg['Subject']}\nDate: {msg['Date']}\n{body.strip()}\n--- End Email ---"



@st.cache_resource

def load_and_process_documents():

Â  Â  # Look in root and in 'uploads' directory

Â  Â  file_patterns = ["**/*.txt", "**/*.md", "**/*.csv", "**/*.pdf", "**/*.eml", "**/*.mbox"]

Â  Â  all_files = [f for pattern in file_patterns for f in glob.glob(pattern, recursive=True)]

Â  Â  docs, file_paths = [], []

Â  Â Â 

Â  Â  if not all_files:

Â  Â  Â  Â  st.info("No documents found. Upload some files to enable document search.")

Â  Â  Â  Â  return docs, file_paths

Â  Â Â 

Â  Â  progress_bar = st.progress(0, text="Loading documents...")

Â  Â Â 

Â  Â  for i, file_path in enumerate(all_files):

Â  Â  Â  Â  file_name = os.path.basename(file_path)

Â  Â  Â  Â  progress_bar.progress((i + 1) / len(all_files), text=f"Processing: {file_name}")

Â  Â  Â  Â  content = ""

Â  Â  Â  Â Â 

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if file_path.endswith('.pdf'):Â 

Â  Â  Â  Â  Â  Â  Â  Â  content = extract_text_from_pdf(file_path)

Â  Â  Â  Â  Â  Â  elif file_path.endswith('.eml'):

Â  Â  Â  Â  Â  Â  Â  Â  with open(file_path, 'rb') as f:Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content = format_email_for_rag(BytesParser(policy=policy.default).parse(f))

Â  Â  Â  Â  Â  Â  elif file_path.endswith('.mbox'):

Â  Â  Â  Â  Â  Â  Â  Â  mbox_content = [format_email_for_rag(msg) for msg in mailbox.mbox(file_path)]

Â  Â  Â  Â  Â  Â  Â  Â  content = "\n\n".join(filter(None, mbox_content))

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with open(file_path, 'r', encoding='utf-8') as f:Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content = f.read()

Â  Â  Â  Â  Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with open(file_path, 'r', encoding='latin-1') as f:Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content = f.read()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if content and len(content.strip()) > 50:

Â  Â  Â  Â  Â  Â  Â  Â  docs.append(clean_text(content))

Â  Â  Â  Â  Â  Â  Â  Â  file_paths.append(file_path)

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  st.warning(f"Could not process {file_name}: {e}")

Â  Â  Â  Â  Â  Â  continue

Â  Â Â 

Â  Â  progress_bar.empty()

Â  Â  if docs:Â 

Â  Â  Â  Â  st.success(f"Successfully loaded and processed {len(docs)} documents!")

Â  Â  return docs, file_paths



@st.cache_resource

def create_search_index(_documents, _file_paths):

Â  Â  if not _documents:

Â  Â  Â  Â  return None, None, [], []

Â  Â  Â  Â Â 

Â  Â  model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

Â  Â  all_chunks, chunk_metadata = [], []

Â  Â Â 

Â  Â  for i, doc in enumerate(_documents):

Â  Â  Â  Â  chunks = smart_chunking(doc, config.chunk_size)

Â  Â  Â  Â  all_chunks.extend(chunks)

Â  Â  Â  Â  chunk_metadata.extend([{'path': _file_paths[i]}] * len(chunks))

Â  Â Â 

Â  Â  if not all_chunks:Â 

Â  Â  Â  Â  return None, None, [], []

Â  Â Â 

Â  Â  with st.spinner("Embedding documents for semantic search..."):

Â  Â  Â  Â  embeddings = model.encode(all_chunks, show_progress_bar=True, normalize_embeddings=True)

Â  Â Â 

Â  Â  index = faiss.IndexFlatIP(embeddings.shape[1])

Â  Â  index.add(embeddings.astype('float32'))

Â  Â  st.success(f"âœ… Search index created with {len(all_chunks)} text chunks.")

Â  Â  return index, model, all_chunks, chunk_metadata



def search_documents(query: str, index, model, chunks: List[str], metadata: List[Dict]) -> List[Dict]:

Â  Â  if not query or index is None:Â 

Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  query_embedding = model.encode([query], normalize_embeddings=True)

Â  Â  scores, indices = index.search(query_embedding.astype('float32'), config.top_k_retrieval)

Â  Â  results = []

Â  Â Â 

Â  Â  for i, idx in enumerate(indices[0]):

Â  Â  Â  Â  if idx != -1 and scores[0][i] > config.similarity_threshold:

Â  Â  Â  Â  Â  Â  Â results.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â 'content': chunks[idx],Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â 'source': metadata[idx]['path'],Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â 'similarity': scores[0][i]

Â  Â  Â  Â  Â  Â  Â })

Â  Â Â 

Â  Â  return sorted(results, key=lambda x: x['similarity'], reverse=True)



# --- Intent Classification ---

def classify_intent(prompt):

Â  Â  prompt = prompt.lower()

Â  Â  if any(kw in prompt for kw in ["how many", "count", "total tickets", "tickets", "jira"]):

Â  Â  Â  Â  return "csv"

Â  Â  if any(kw in prompt for kw in ['risk', 'failure', 'alert', 'schedule', 'calendar', 'upcoming', 'maintenance']):

Â  Â  Â  Â  return "maintenance"

Â  Â  return "search"



# --- Initialization ---

try:

Â  Â  maintenance_pipeline = MaintenancePipeline()

Â  Â  docs, paths = load_and_process_documents()

Â  Â  if docs:

Â  Â  Â  Â  search_index, model, all_chunks, chunk_meta = create_search_index(docs, paths)

Â  Â  Â  Â  search_args = {'index': search_index, 'model': model, 'chunks': all_chunks, 'metadata': chunk_meta}

Â  Â  else:

Â  Â  Â  Â  search_index, model, all_chunks, chunk_meta, search_args = None, None, [], [], {}

Â  Â  Â  Â  if not docs:

Â  Â  Â  Â  Â  Â  st.info("ğŸ“ No documents found. Document search will be limited until you upload files.")

except Exception as e:

Â  Â  st.error(f"An error occurred during initialization: {e}")

Â  Â  search_index, search_args = None, {}



if "messages" not in st.session_state:

Â  Â  st.session_state.messages = [{"role": "assistant", "content": "Hi! How can I help you today?", "timestamp": datetime.now().strftime("%H:%M")}]





# --- Sidebar: Quick Actions, Upload, Settings ---

with st.sidebar:

Â  Â  st.title("ğŸ¤– AI Support Assistant")

Â  Â  st.markdown("**Quick Actions:**")



Â  Â  # Handler function for quick actions

Â  Â  def handle_quick_action(action_type: str):

Â  Â  Â  Â  if action_type == "alerts":

Â  Â  Â  Â  Â  Â  prompt = "Show high-risk equipment alerts"

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  prompt = "Show upcoming maintenance schedule"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  now = datetime.now().strftime("%H:%M")

Â  Â  Â  Â  # Add user message

Â  Â  Â  Â  st.session_state.messages.append({

Â  Â  Â  Â  Â  Â  "role": "user",

Â  Â  Â  Â  Â  Â  "content": prompt,

Â  Â  Â  Â  Â  Â  "timestamp": now

Â  Â  Â  Â  })

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Generate AI response context

Â  Â  Â  Â  context = get_maintenance_context_with_actions(

Â  Â  Â  Â  Â  Â  prompt,Â 

Â  Â  Â  Â  Â  Â  maintenance_pipeline,

Â  Â  Â  Â  Â  Â  search_documents if search_args else None,

Â  Â  Â  Â  Â  Â  search_args if search_args else None

Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  # Generate and add AI response

Â  Â  Â  Â  response_stream = generate_response_stream(prompt, st.session_state.messages, context)

Â  Â  Â  Â  full_response = "".join([chunk for chunk in response_stream])

Â  Â  Â  Â  st.session_state.messages.append({

Â  Â  Â  Â  Â  Â  "role": "assistant",

Â  Â  Â  Â  Â  Â  "content": full_response,

Â  Â  Â  Â  Â  Â  "timestamp": now

Â  Â  Â  Â  })

Â  Â  Â  Â  # Force a rerun to show the new messages

Â  Â  Â  Â  st.rerun()



Â  Â  col1, col2 = st.columns(2)

Â  Â  with col1:

Â  Â  Â  Â  if st.button("ğŸ“Š Risk Alerts", key="risk_btn", help="Show equipment with the highest failure probability."):

Â  Â  Â  Â  Â  Â  handle_quick_action("alerts")

Â  Â  with col2:

Â  Â  Â  Â  if st.button("ğŸ“… Maintenance", key="maint_btn", help="Show the upcoming maintenance schedule."):

Â  Â  Â  Â  Â  Â  handle_quick_action("maintenance")



Â  Â  # System status

Â  Â  st.markdown("---")

Â  Â  st.markdown("**System Status:**")

Â  Â  high_risk_count = len(maintenance_pipeline.get_equipment_by_risk('HIGH'))

Â  Â  upcoming_count = len(maintenance_pipeline.get_maintenance_schedule(7))

Â  Â  st.write(f"ğŸš¨ {high_risk_count} high risk items")

Â  Â  st.write(f"ğŸ“… {upcoming_count} tasks due this week")

Â  Â Â 

Â  Â  # File upload

Â  Â  st.markdown("---")

Â  Â  st.info("Upload new documents below:")

Â  Â  uploaded_files = st.file_uploader("Add files", accept_multiple_files=True,Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  type=['txt', 'md', 'csv', 'pdf', 'eml', 'mbox'])

Â  Â  if uploaded_files:

Â  Â  Â  Â  upload_dir = "uploads"

Â  Â  Â  Â  os.makedirs(upload_dir, exist_ok=True)

Â  Â  Â  Â  for uploaded_file in uploaded_files:

Â  Â  Â  Â  Â  Â  file_path = os.path.join(upload_dir, uploaded_file.name)

Â  Â  Â  Â  Â  Â  with open(file_path, "wb") as f:

Â  Â  Â  Â  Â  Â  Â  Â  f.write(uploaded_file.getbuffer())

Â  Â  Â  Â  st.success(f"âœ… {len(uploaded_files)} files uploaded!")

Â  Â  Â  Â  if st.button("ğŸ”„ Reload App", help="Reload to re-index uploaded documents"):

Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â Â 

Â  Â  # Settings

Â  Â  st.markdown("---")

Â  Â  st.markdown("**Settings:**")

Â  Â  chunk_size = st.slider("Chunk size for document splitting", 200, 1000,Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.settings["chunk_size"], 100)

Â  Â  st.session_state.settings["chunk_size"] = chunk_size

Â  Â Â 

Â  Â  theme = st.radio("Theme", ["light", "dark"],Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â index=0 if st.session_state.settings["theme"]=="light" else 1)

Â  Â  st.session_state.settings["theme"] = theme



# --- Improved Chat Bubble with timestamps ---

def chat_bubble(content, is_user=False, timestamp=None):

Â  Â  theme = st.session_state.settings.get("theme", "light")

Â  Â  if theme == "dark":

Â  Â  Â  Â  user_color = "#3A6351"

Â  Â  Â  Â  assistant_color = "#222831"

Â  Â  Â  Â  text_color = "#fff"

Â  Â  Â  Â  border = "1px solid #393E46"

Â  Â  else:

Â  Â  Â  Â  user_color = "#DCF8C6"

Â  Â  Â  Â  assistant_color = "#F1F0F0"

Â  Â  Â  Â  text_color = "#222"

Â  Â  Â  Â  border = "1px solid #ddd"

Â  Â Â 

Â  Â  color = user_color if is_user else assistant_color

Â  Â  avatar_url = (

Â  Â  Â  Â  "https://cdn-icons-png.flaticon.com/512/1946/1946429.png" if is_user

Â  Â  Â  Â  else "https://cdn-icons-png.flaticon.com/512/4712/4712035.png"

Â  Â  )

Â  Â  name = "You" if is_user else "AI Assistant"

Â  Â  if not timestamp:

Â  Â  Â  Â  timestamp = datetime.now().strftime("%H:%M")

Â  Â Â 

Â  Â  # Sanitize content for HTML display

Â  Â  content = content.replace("<", "&lt;").replace(">", "&gt;")



Â  Â  st.markdown(

Â  Â  Â  Â  f"""

Â  Â  Â  Â  <div style='display:flex;flex-direction:{"row-reverse" if is_user else "row"};align-items:flex-end;margin:12px 0;'>

Â  Â  Â  Â  Â  Â  <img src="{avatar_url}" width="36" height="36" style="border-radius:50%;margin:0 8px;flex-shrink:0;">

Â  Â  Â  Â  Â  Â  <div style='background-color:{color};

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color:{text_color};

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  padding:14px 16px;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  border-radius:12px;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max-width:70%;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  min-width:80px;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  border:{border};

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  box-shadow:0 2px 8px rgba(0,0,0,0.07);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  position:relative;'>

Â  Â  Â  Â  Â  Â  Â  Â  <div style='font-size:13px;font-weight:bold;opacity:0.7;margin-bottom:6px;'>{name}</div>

Â  Â  Â  Â  Â  Â  Â  Â  <div style='font-size:16px;line-height:1.4;'>{content}</div>

Â  Â  Â  Â  Â  Â  Â  Â  <div style='font-size:11px;opacity:0.5;text-align:right;margin-top:8px;'>{timestamp}</div>

Â  Â  Â  Â  Â  Â  </div>

Â  Â  Â  Â  </div>

Â  Â  Â  Â  """, unsafe_allow_html=True

Â  Â  )



# --- Chat History Display ---

for message in st.session_state.messages:

Â  Â  chat_bubble(

Â  Â  Â  Â  message["content"],

Â  Â  Â  Â  is_user=(message["role"] == "user"),

Â  Â  Â  Â  timestamp=message.get("timestamp")

Â  Â  )



# --- Chat Input and Tool Routing ---

if prompt := st.chat_input("Ask a question..."):

Â  Â  now = datetime.now().strftime("%H:%M")

Â  Â  # Add user message and display immediately

Â  Â  st.session_state.messages.append({"role": "user", "content": prompt, "timestamp": now})

Â  Â  chat_bubble(prompt, is_user=True, timestamp=now)

Â  Â Â 

Â  Â  with st.spinner("Thinking..."):

Â  Â  Â  Â  intent = classify_intent(prompt)

Â  Â  Â  Â  context = ""

Â  Â  Â  Â Â 

Â  Â  Â  Â  if intent == "csv":

Â  Â  Â  Â  Â  Â  context = analyze_csv_data(prompt)

Â  Â  Â  Â  elif intent == "maintenance":

Â  Â  Â  Â  Â  Â  context = get_maintenance_context_with_actions(

Â  Â  Â  Â  Â  Â  Â  Â  prompt,Â 

Â  Â  Â  Â  Â  Â  Â  Â  maintenance_pipeline,Â 

Â  Â  Â  Â  Â  Â  Â  Â  search_documents if search_args else None,Â 

Â  Â  Â  Â  Â  Â  Â  Â  search_args if search_args else None

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  else:Â  # "search" intent

Â  Â  Â  Â  Â  Â  if search_args:

Â  Â  Â  Â  Â  Â  Â  Â  search_results = search_documents(prompt, **search_args)

Â  Â  Â  Â  Â  Â  Â  Â  if search_results:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context += "### Relevant Information from Documents:\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for result in search_results[:3]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source = os.path.basename(result['source'])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content_preview = result['content'].strip().replace('\n', ' ')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context += f"- **From `{source}`**: \"{content_preview[:300]}...\"\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context += "\n*Citations provided above.*"

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context = "No relevant documents found for your query."

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  context = "Document search is not available. Please upload documents to enable this feature."

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Generate fallback context if nothing found

Â  Â  Â  Â  if not context.strip():

Â  Â  Â  Â  Â  Â  high_risk_count = len(maintenance_pipeline.get_equipment_by_risk('HIGH'))

Â  Â  Â  Â  Â  Â  upcoming_maintenance = len(maintenance_pipeline.get_maintenance_schedule(7))

Â  Â  Â  Â  Â  Â  context = f"""I couldn't find specific information for that query.



**What you can ask about:**

- Document contents (upload files first)

- Maintenance alerts and schedules

- Ticket counts from CSV files (e.g., "How many Device Faulty issues?")



**Current System Status:** ğŸš¨ {high_risk_count} high risk equipment itemsÂ Â 

ğŸ“… {upcoming_maintenance} maintenance tasks due this week"""



Â  Â  Â  Â  # Check if we should render interactive content instead of regular AI response

Â  Â  Â  Â  # NOTE: This is a placeholder for future interactive UI elements

Â  Â  Â  Â  if context and ("INTERACTIVE_ALERTS" in context or "INTERACTIVE_SCHEDULE" in context):

Â  Â  Â  Â  Â  Â  # Render interactive content directly

Â  Â  Â  Â  Â  Â  # render_interactive_content(context) # This function would be defined elsewhere

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Add a simple message to chat history

Â  Â  Â  Â  Â  Â  if "INTERACTIVE_ALERTS" in context:

Â  Â  Â  Â  Â  Â  Â  Â  summary_msg = "Displayed interactive high-risk equipment alerts with action buttons."

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  summary_msg = "Displayed interactive maintenance schedule with management options."

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role": "assistant", "content": summary_msg, "timestamp": now})

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  # Regular AI response generation

Â  Â  Â  Â  Â  Â  response_placeholder = st.empty()

Â  Â  Â  Â  Â  Â  full_response = ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  response_stream = generate_response_stream(prompt, st.session_state.messages, context)

Â  Â  Â  Â  Â  Â  Â  Â  for chunk in response_stream:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_response += chunk

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with response_placeholder.container():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chat_bubble(full_response + " â–Œ", is_user=False, timestamp=now)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Final update to remove cursor

Â  Â  Â  Â  Â  Â  Â  Â  with response_placeholder.container():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chat_bubble(full_response, is_user=False, timestamp=now)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  full_response = f"I apologize, but I encountered an error: {str(e)}"

Â  Â  Â  Â  Â  Â  Â  Â  with response_placeholder.container():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chat_bubble(full_response, is_user=False, timestamp=now)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Add to chat history

Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role": "assistant", "content": full_response, "timestamp": now})



# --- Footer ---

st.markdown("---")

st.markdown("*AI Support Assistant - Powered by Streamlit & Gemini*")

